 - ensemble means Group
 - Multiple models group together and bring forth the model that is more accurate.
 - Ensemble methods are techniques that create multiple models and then combine 
   them to produce improved results.

 - Bagging: making models in parallel.
 - Boosting: Making models in series or sequential.
 - In each successive model, the weights are adjusted based on learning of previous model.

 - ADA Boost : Adaptive boosting.

 - randomly select the training data and build the 1st model.
 - use the training data to test the model.

 - The training data which does not fit in the model, will be used along with other training data, to build the second model.

 

 - usually produces more accurate solutions than a single model would

 - Widely known methods of ensemble: 
 - voting, 
 - stacking, 
 - bagging and 
 - boosting.
 
Two types of ensemble methods.

1.Averaging method/Bagging:-
In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. 
Example: Bagging ,Forest randomized tree.

2.Boosting:-
n boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.

Example:  AdaBoost, Gradient Tree Boosting.

Bagging:-
Bagging is a technique used to reduce the variance of our predictions by
combining the result of multiple classifiers modeled on different sub-samples of the same data set.

BaggingClassifier and BaggingRegressor are 2 module in scikit learn.

Example:
 from sklearn.ensemble import BaggingClassifier
 from sklearn.neighbors import KNeighborsClassifier
 bagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)
 
max_samples and max_features control the size of the subsets (in terms of samples and features)

Forests of randomized trees:-
1.RandomForestClassifier
2.RandomForestRegressor
Example:-

from sklearn.ensemble import RandomForestClassifier
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = RandomForestClassifier(n_estimators=10)
clf = clf.fit(X, Y)

Extremely Randomized Trees:-
ExtraTreesClassifier
ExtraTreesRegressor

Example:-
***********************************
 from sklearn.model_selection import cross_val_score
 from sklearn.datasets import make_blobs
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.ensemble import ExtraTreesClassifier
 from sklearn.tree import DecisionTreeClassifier

 X, y = make_blobs(n_samples=10000, n_features=10, centers=100,random_state=0)

 clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)
 scores = cross_val_score(clf, X, y)
 scores.mean()                             
 0.97

 clf = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)
 scores = cross_val_score(clf, X, y)
 scores.mean()                             
0.999

 clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)
 scores = cross_val_score(clf, X, y)
 scores.mean() > 0.999
************************************************ 
n_estimators is the no of trees in the forest.
If n_jobs=k then computations are partitioned into k jobs, and run on k cores of the machine.

AdaBoost:-
AdaBoostClassifier
AdaBoostRegressor

 ***********************************************

 from sklearn.model_selection import cross_val_score
 from sklearn.datasets import load_iris
 from sklearn.ensemble import AdaBoostClassifier

 iris = load_iris()
 clf = AdaBoostClassifier(n_estimators=100)
 scores = cross_val_score(clf, iris.data, iris.target)
 scores.mean()                             
 0.9

Gradient Tree Boosting:-
GradientBoostingClassifier
GradientBoostingRegressor

from sklearn.datasets import make_hastie_10_2
 from sklearn.ensemble import GradientBoostingClassifier

 X, y = make_hastie_10_2(random_state=0)
 X_train, X_test = X[:2000], X[2000:]
 y_train, y_test = y[:2000], y[2000:]

 clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)
 clf.score(X_test, y_test)                 
 0.913...
 ****************************************************
 
Voting Classifier:-
*****************************************************
 from sklearn import datasets
 from sklearn.model_selection import cross_val_score
 from sklearn.linear_model import LogisticRegression
 from sklearn.naive_bayes import GaussianNB
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.ensemble import VotingClassifier

 iris = datasets.load_iris()
 X, y = iris.data[:, 1:3], iris.target

 clf1 = LogisticRegression(random_state=1)
 clf2 = RandomForestClassifier(random_state=1)
 clf3 = GaussianNB()

 eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')

 for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
Accuracy: 0.93 (+/- 0.05) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [naive Bayes]
Accuracy: 0.95 (+/- 0.05) [Ensemble]
*********************************************************************************
