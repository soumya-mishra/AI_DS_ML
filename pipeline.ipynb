{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b23858a7-8f9f-4fb7-bfcf-0f1c9a2d5bba",
    "_uuid": "125ba73bccf909bf24a1cfee0d6c0bd772d95d06"
   },
   "source": [
    " \n",
    "* Pipelines are a way to streamline a lot of the routine processes,\n",
    "* encapsulating little pieces of logic into one function call, \n",
    "* which makes it easier to actually do modeling instead just writing a bunch of code.\n",
    "* Pipelines allow for experiments,\n",
    "* and for a dataset like this that only has the text as a feature,\n",
    "* We are going to need to do a lot of experiments.\n",
    "* Plus, when your modeling gets really complicated,-\n",
    "* it's sometimes hard to see if you have any data leakage hiding somewhere.\n",
    "* Pipelines are set up with the fit/transform/predict functionality, \n",
    "* so we can fit a whole pipeline to the training data and transform to the test data, \n",
    "* without having to do it individually for each thing you do. \n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0289247b-e183-4b83-9a4a-777c1ccd2cbc",
    "_uuid": "282f761b71584453b7c57203edfcc8677d9738a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:/Users/Downloads/train_data/train_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author\n",
       "id                                                               \n",
       "id26305  This process, however, afforded me no means of...    EAP\n",
       "id17569  It never once occurred to me that the fumbling...    HPL\n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=0)\n",
    "df.set_index('id', inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d72d93c-d319-4d4e-ad04-ee0d153f7fb3",
    "_uuid": "2686ff0796119ece5ada79c8a9a3c35d323e9420"
   },
   "source": [
    "## Preprocessing and Feature Engineering\n",
    "\n",
    "- To make it easier to replicate on the submission data, we will encapsulate the logic into a function.\n",
    "\n",
    "- Note, all of this preprocessing is standard stuff, and does not depend on the data it's processing on, so it's ok to do this now. Things like count vectorization and numeric scaling depend on the data it's run on, so that part must be done differently. We will get to that later.\n",
    "\n",
    "- For now, we will count the number of words in each row, the number of characters, the number of non stop words, and the number of commas, since who knows, maybe using commas helps build suspense??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "63e391e6-e238-454f-b93c-b2f249d16efc",
    "_uuid": "2596bb126e8ee018a6940ae31c8aa89cc0d84e8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>processed</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>commas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>this process however afforded me no means of a...</td>\n",
       "      <td>224</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>6.380952</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>it never once occurred to me that the fumbling...</td>\n",
       "      <td>70</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>in his left hand was a gold snuff box from whi...</td>\n",
       "      <td>195</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>5.947368</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>how lovely is spring as we looked from windsor...</td>\n",
       "      <td>202</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>6.476190</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>finding nothing else not even gold the superin...</td>\n",
       "      <td>170</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>7.187500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  \\\n",
       "id                                                                  \n",
       "id26305  This process, however, afforded me no means of...    EAP   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                                 processed  length  words  \\\n",
       "id                                                                          \n",
       "id26305  this process however afforded me no means of a...     224     41   \n",
       "id17569  it never once occurred to me that the fumbling...      70     14   \n",
       "id11008  in his left hand was a gold snuff box from whi...     195     36   \n",
       "id27763  how lovely is spring as we looked from windsor...     202     34   \n",
       "id12958  finding nothing else not even gold the superin...     170     27   \n",
       "\n",
       "         words_not_stopword  avg_word_length  commas  \n",
       "id                                                    \n",
       "id26305                  21         6.380952       4  \n",
       "id17569                   6         6.166667       0  \n",
       "id11008                  19         5.947368       4  \n",
       "id27763                  21         6.476190       3  \n",
       "id12958                  16         7.187500       2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "#creating a function to encapsulate preprocessing, to mkae it easy to replicate on  submission data\n",
    "def processing(df):\n",
    "    #lowering and removing punctuation\n",
    "    df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n",
    "    \n",
    "    #numerical feature engineering\n",
    "    #total length of sentence\n",
    "    df['length'] = df['processed'].apply(lambda x: len(x))\n",
    "    #get number of words\n",
    "    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n",
    "    df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n",
    "    #get the average word length\n",
    "    df['avg_word_length'] = df['processed'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0)\n",
    "    #get the average word length\n",
    "    df['commas'] = df['text'].apply(lambda x: x.count(','))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "df = processing(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f65b065-59e9-42ea-a76e-9a38a9e69109",
    "_uuid": "e477b8cbf9fc8a15c9f4aa13908314350caaa95b"
   },
   "source": [
    "### Creating a Pipeline\n",
    "\n",
    "- Sklearn's pipeline functionality makes it easier to repeat commonly occuring steps in our modeling process.\\\n",
    "- Similar to the processing function I made above, it provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything.\n",
    "\n",
    "- let's build the pipelines up from the bottom.\\\n",
    "- since pipelines are made from pipelines, it's useful to see how they build on each other.\n",
    "\n",
    "First step, split your data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "27efeecb-8feb-41c1-96d8-7bc21b452e7b",
    "_uuid": "23b0a8d116caf454325f63af4b108e74023ca8de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>commas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id19417</th>\n",
       "      <td>this panorama is indeed glorious and i should ...</td>\n",
       "      <td>91</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id09522</th>\n",
       "      <td>there was a simple natural earnestness about h...</td>\n",
       "      <td>240</td>\n",
       "      <td>44</td>\n",
       "      <td>18</td>\n",
       "      <td>6.277778</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id22732</th>\n",
       "      <td>who are you pray that i duc de lomelette princ...</td>\n",
       "      <td>387</td>\n",
       "      <td>74</td>\n",
       "      <td>38</td>\n",
       "      <td>5.552632</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id10351</th>\n",
       "      <td>he had gone in the carriage to the nearest tow...</td>\n",
       "      <td>118</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>5.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id24580</th>\n",
       "      <td>there is no method in their proceedings beyond...</td>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 processed  length  words  \\\n",
       "id                                                                          \n",
       "id19417  this panorama is indeed glorious and i should ...      91     18   \n",
       "id09522  there was a simple natural earnestness about h...     240     44   \n",
       "id22732  who are you pray that i duc de lomelette princ...     387     74   \n",
       "id10351  he had gone in the carriage to the nearest tow...     118     24   \n",
       "id24580  there is no method in their proceedings beyond...      71     13   \n",
       "\n",
       "         words_not_stopword  avg_word_length  commas  \n",
       "id                                                    \n",
       "id19417                   6         6.666667       1  \n",
       "id09522                  18         6.277778       4  \n",
       "id22732                  38         5.552632       9  \n",
       "id10351                  11         5.363636       0  \n",
       "id24580                   5         7.000000       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features= [c for c in df.columns.values if c  not in ['id','text','author']]\n",
    "numeric_features= [c for c in df.columns.values if c  not in ['id','text','author','processed']]\n",
    "target = 'author'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length', 'words', 'words_not_stopword', 'avg_word_length', 'commas']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b712489b-156e-48bb-a2a2-53eac0060cc9",
    "_uuid": "74efdcc9ffe37aac7901fbb63e1a1feede3602cb"
   },
   "source": [
    "Now for the tricky parts.\n",
    "\n",
    "First thing I want to do is define how to process my variables. The standard preprocessing apply the same preprocessing to the whole dataset, but in cases where you have heterogeneous data, this doesn't quite work. So first thing I'm going to do is create a selector transformer that simply returns the one column in the dataset by the key value I pass. \n",
    "\n",
    "I was having difficulty getting the selector to play nicely, so I made two different selectors for either text or numeric columns. The return type is different, but other than that they work the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "66f0363d-1414-4d23-87fa-a0190c0f6a3a",
    "_uuid": "2d8983e1d86a9d1323b0bde3083c6fe2e2650378"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6a864df-d1f5-4199-aba6-658c6cce2c49",
    "_uuid": "f0cbfcf81d77373d88bd24522020c8469abeb9a0"
   },
   "source": [
    "To see how this is used, let's actually run it on one column.\n",
    "\n",
    "I'm going to call it on the text column and transform it with another step. But again, pipelines are all about encapsulating several steps, so I'm going to make a mini pipeline that consists of two steps: first grab just that column from the dataset, then perform tf-idf on just that column and return the results.\n",
    "\n",
    "To make a pipeline, just pass an array of tuples of the format (name, object). The first part is the name of the action, and the second is the actual object. So this pipeline consists of \"selecting\" and then \"tfidf-ing\" a column.\n",
    "\n",
    "To execute, use it just like any other transformer. You can call text.fit() to fit to training data, text.transform() to apply it to training data, or text.fit_transform() to do both. \n",
    "\n",
    "Since it's text, it will return a sparse matrix, but we can see that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "55de63e6-8d2f-478f-9126-6e7b0546207c",
    "_uuid": "2caac353c114eb6cc7e493eecfa90639cadb5e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13117x21516 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 148061 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='processed')),\n",
    "                ('tfidf', TfidfVectorizer( stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dec38474-3a0a-45d3-a52f-f99dd72f0f2a",
    "_uuid": "17e07518d728d90fe52483eeec70b7bdda9b8c12"
   },
   "source": [
    "Since our data is heterogeneous, we might want to do something else on numeric data, so let's build a mini pipeline for that too.\n",
    "\n",
    "This transformer will be a simple scaler. Since our data is mixed, we must apply it column by column. Let's make one to process the \"length\" variable I made above. Just like the text one, we combine two steps, first selecting the column, then transforming the column, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "ea77456b-f7cc-4480-97a5-4b4c819de7a1",
    "_uuid": "eb59242f760520108b9b8f73c108f584af3130f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.50769254],\n",
       "       [ 0.88000324],\n",
       "       [ 2.24907223],\n",
       "       ...,\n",
       "       [-0.46112557],\n",
       "       [-0.14447015],\n",
       "       [-0.39593181]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "length.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "71f2c594-9bd8-4d4e-bd3c-7316a604bcdf",
    "_uuid": "ec08bbc51a6226c4d725936b36243b819bd43d1a"
   },
   "source": [
    "We can see that the transformer pipeline returns a matrix for the column it's called on, so now all that's left to do is join the results from several transformed variables into a single dataset. I'll go ahead and make a pipeline for every variable in the data, then join them all together. \n",
    "\n",
    "First, I'll transform all the numeric columns with the standard scaler, but of course you can change the scaler for any column as you desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "80d80de4-bede-4cdf-8047-e3bb878c6609",
    "_uuid": "93fde50658fe3a3ce9b481d66f8ff39570a6be10"
   },
   "outputs": [],
   "source": [
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "150d3ce6-9fed-4ce0-a3e9-8f955ad7874d",
    "_uuid": "2a709500025294ef8a4cfda02f7a7d02a96ff61b"
   },
   "source": [
    "To make a pipeline from all of our pipelines, we do the same thing, but now we use a FeatureUnion to join the feature processing pipelines.\n",
    "\n",
    "The syntax is the same as a regular pipeline, it's just an array of tuple, with the (name, object) format. \n",
    "\n",
    "The feature union itself is not a pipeline, it's just a union, so you need to do *one more step* to make it useable: pass it to a pipeline, with the same structure, an array of tuples, with the simple (name, object) format. . As you can see, we get a pipeline-ception going on the more complex you get! \n",
    "\n",
    "You can then apply all those transformations at once with a single fit, transform, or fit_transform call. Nice, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "95d16585-367f-4e74-98fd-bd266e19a672",
    "_uuid": "65f49fd19c57ba619518b2a0de615e08cb4f3c5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13117x21521 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 213646 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas)])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e759f03-b2fd-4dcc-893e-ab00bbe0a128",
    "_uuid": "c1e3d4b661d5a48ee886ddbee613e384f9525d94"
   },
   "source": [
    "To add a model to the mix and generate predictions as well, you can add a model at the end of the pipeline. The syntax is, you guessed it, an array of tuples, merging the transformations with a model. \n",
    "\n",
    "We can see the raw accuracy is at 63%. Not bad for a start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "84b4ce5d-c4a3-46f7-903f-c4c2d7518ecd",
    "_uuid": "acaeeee80c58268f3b21d9d2c1cd7a9a3b5fcd21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I324158\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "preds = pipeline.predict(X_test)\n",
    "# np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP', 'EAP', 'EAP', ..., 'EAP', 'EAP', 'MWS'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "637a6944-320b-4d17-8623-2baa449b2903",
    "_uuid": "eb832df858b4a2e078857de3dc9fc8e302c41bf6"
   },
   "source": [
    "# Bringing It All Together\n",
    "\n",
    "Preprocessing is great, but most likely, you actually want to model something too. To do that, we just need to add a classifier at the end of the pipeline. Here I'm going to make a pipeline that does all the processing I made above, then passes it to a Random Forest. As you might guess, this requires passing an array of tuples to the pipeline, with the (name, object) structure. \n",
    "\n",
    "To put it to use, we just fit and predict as if it was a regular classifier. First we fit on the training, then predict on the test data, and then see how well it did:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10444a55-f32b-41cd-8dbd-c1ea63b4c9cb",
    "_uuid": "66c104071c5e7bab167da53c4747610f6e568a39"
   },
   "source": [
    "# Cross Validation To Find The Best Pipeline\n",
    "\n",
    "That alone should give you enough flexibility to create some rather complex pipelines. But we're on a role, let's keep going.\n",
    "\n",
    "What if I wanted to do cross validation on my pipeline? How many trees should I use on my classifier? How deep should I go? Or even more complicated, how many words should I use in my tf-idf transform? Should I include stop words? Pipelines allow you to do that with just a few more lines.\n",
    "\n",
    "Cross validation is all about figuring out what the best hyperparameters of the data set is. To see the list of all the possible things you could fine tune, call get_params().keys() on your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "6ce23a11-a9e8-455e-857e-249615b89066",
    "_uuid": "c0bfe59aaf80d0aacf57eced7fad9252a24a6a4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'features', 'classifier', 'features__n_jobs', 'features__transformer_list', 'features__transformer_weights', 'features__verbose', 'features__text', 'features__length', 'features__words', 'features__words_not_stopword', 'features__avg_word_length', 'features__commas', 'features__text__memory', 'features__text__steps', 'features__text__verbose', 'features__text__selector', 'features__text__tfidf', 'features__text__selector__key', 'features__text__tfidf__analyzer', 'features__text__tfidf__binary', 'features__text__tfidf__decode_error', 'features__text__tfidf__dtype', 'features__text__tfidf__encoding', 'features__text__tfidf__input', 'features__text__tfidf__lowercase', 'features__text__tfidf__max_df', 'features__text__tfidf__max_features', 'features__text__tfidf__min_df', 'features__text__tfidf__ngram_range', 'features__text__tfidf__norm', 'features__text__tfidf__preprocessor', 'features__text__tfidf__smooth_idf', 'features__text__tfidf__stop_words', 'features__text__tfidf__strip_accents', 'features__text__tfidf__sublinear_tf', 'features__text__tfidf__token_pattern', 'features__text__tfidf__tokenizer', 'features__text__tfidf__use_idf', 'features__text__tfidf__vocabulary', 'features__length__memory', 'features__length__steps', 'features__length__verbose', 'features__length__selector', 'features__length__standard', 'features__length__selector__key', 'features__length__standard__copy', 'features__length__standard__with_mean', 'features__length__standard__with_std', 'features__words__memory', 'features__words__steps', 'features__words__verbose', 'features__words__selector', 'features__words__standard', 'features__words__selector__key', 'features__words__standard__copy', 'features__words__standard__with_mean', 'features__words__standard__with_std', 'features__words_not_stopword__memory', 'features__words_not_stopword__steps', 'features__words_not_stopword__verbose', 'features__words_not_stopword__selector', 'features__words_not_stopword__standard', 'features__words_not_stopword__selector__key', 'features__words_not_stopword__standard__copy', 'features__words_not_stopword__standard__with_mean', 'features__words_not_stopword__standard__with_std', 'features__avg_word_length__memory', 'features__avg_word_length__steps', 'features__avg_word_length__verbose', 'features__avg_word_length__selector', 'features__avg_word_length__standard', 'features__avg_word_length__selector__key', 'features__avg_word_length__standard__copy', 'features__avg_word_length__standard__with_mean', 'features__avg_word_length__standard__with_std', 'features__commas__memory', 'features__commas__steps', 'features__commas__verbose', 'features__commas__selector', 'features__commas__standard', 'features__commas__selector__key', 'features__commas__standard__copy', 'features__commas__standard__with_mean', 'features__commas__standard__with_std', 'classifier__bootstrap', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__min_impurity_decrease', 'classifier__min_impurity_split', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dbfb2342-2817-4ced-a609-9938562e01c5",
    "_uuid": "309d37e42e11abfdddc127acebe8d2bcb2b452df"
   },
   "source": [
    "Obviously don't be crazy, cross validation takes a while to run, and the more options you select, the longer it takes. But to give you an idea on how these work together, to test out the different combinations, define a dictionary with the settings you want, with the key being the pipeline's parameter key name, and the value being an array of all the settings you want to apply.\n",
    "\n",
    "After the dictionary is made, call GridSearchCV on your pipeline, passing the dictionary and the number of folds you want to use. \n",
    "\n",
    "Here's an example with a few settings on 5 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "602a1465-ff99-44f2-9d4d-4b6847afba2e",
    "_uuid": "7d2b420ef99bf024a2bd057f97538ab471674fd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('selector',\n",
       "                                                                                         TextSelector(key='processed')),\n",
       "                                                                                        ('tfidf',\n",
       "                                                                                         TfidfVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.float64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input=...\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=42,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classifier__max_depth': [50, 70],\n",
       "                         'classifier__min_samples_leaf': [1, 2],\n",
       "                         'features__text__tfidf__max_df': [0.9, 0.95],\n",
       "                         'features__text__tfidf__ngram_range': [(1, 1),\n",
       "                                                                (1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = { 'features__text__tfidf__max_df': [0.9, 0.95],\n",
    "                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "                   'classifier__max_depth': [50, 70],\n",
    "                    'classifier__min_samples_leaf': [1,2]\n",
    "                  }\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "16c7c9d1-4c17-4619-a2f1-138de837e1cf",
    "_uuid": "9ad824ce203554cdda13502640f80e8d2c4ddc9d"
   },
   "source": [
    "If you want to see which settings won, you can do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d1ee7fb2-c946-41b9-adbb-3ac449b04f7e",
    "_uuid": "bc5e2c7551604acbb3deb49fa662f21710268bf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': 70,\n",
       " 'classifier__min_samples_leaf': 2,\n",
       " 'features__text__tfidf__max_df': 0.9,\n",
       " 'features__text__tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0b2899ab-ca1e-4814-a81c-258aa28224cf",
    "_uuid": "db2f77237f9026de72b29384a870a7d1adab5cf3"
   },
   "source": [
    "What's really convenient is you can call refit to automatically fit the pipeline on all of the training data with the best_params_setting applied!\n",
    "\n",
    "Then applying it to the test data is the same as before.\n",
    "\n",
    "Not much of an improvement, but at least now we can go back and easily change out the individual pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "23ed5f54-0d58-4a2d-ad7a-ca8d80f5574c",
    "_uuid": "d9eba0d0ee5dce9f777c633881bcdd19eda6c9c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6425255338904364"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#refitting on entire training data using best settings\n",
    "clf.refit\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)\n",
    "\n",
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2f6094d1-2f14-4188-930a-d67200f49738",
    "_uuid": "f3e4177bc191ee0d529bf84ef60fe89c34b4f6bd"
   },
   "source": [
    "# Final Predictions\n",
    "\n",
    "To generate submission results, you just need to do the preprocessing on the submission data, then call the pipeline with the predict_proba call, since we want to know all the probabilities, not just the label.\n",
    "\n",
    "The only tricky part for the submission is we need the class names as the column values. To access it, you must call clf.best_estimator_.named_steps['classifier'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "7e8526e3-f855-4ace-b6fb-2e00dbdf8d0b",
    "_uuid": "1b7038b1d5a30479213799839531867afa91da5e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id02310</th>\n",
       "      <td>0.308413</td>\n",
       "      <td>0.278221</td>\n",
       "      <td>0.413366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id24541</th>\n",
       "      <td>0.636757</td>\n",
       "      <td>0.161106</td>\n",
       "      <td>0.202137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00134</th>\n",
       "      <td>0.328440</td>\n",
       "      <td>0.449705</td>\n",
       "      <td>0.221855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27757</th>\n",
       "      <td>0.647944</td>\n",
       "      <td>0.167825</td>\n",
       "      <td>0.184231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id04081</th>\n",
       "      <td>0.528074</td>\n",
       "      <td>0.206907</td>\n",
       "      <td>0.265020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              EAP       HPL       MWS\n",
       "id                                   \n",
       "id02310  0.308413  0.278221  0.413366\n",
       "id24541  0.636757  0.161106  0.202137\n",
       "id00134  0.328440  0.449705  0.221855\n",
       "id27757  0.647944  0.167825  0.184231\n",
       "id04081  0.528074  0.206907  0.265020"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/test.csv')\n",
    "\n",
    "#preprocessing\n",
    "submission = processing(submission)\n",
    "predictions = clf.predict_proba(submission)\n",
    "\n",
    "preds = pd.DataFrame(data=predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)\n",
    "\n",
    "#generating a submission file\n",
    "result = pd.concat([submission[['id']], preds], axis=1)\n",
    "result.set_index('id', inplace = True)\n",
    "result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
