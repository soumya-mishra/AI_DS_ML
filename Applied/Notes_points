
Exploratory data analysis
imbalance & Banlanced dataset

2d scatter plot 
3d Scatter plot | what about 4d 5d - can human visualize ?- no
1D scaatter plot - can not read( can not see how many points are there for each class)

pair plot  -  seaborne  -  multiple plots for each pair of columns . ignore diagonal element 
Write observation after plotting -  simple sentence - numeric value
more than 10 features - difficult to visualize and takes time

Univriate Analysis:
Histograms - PDF , CDF , Bins ( Binning )
1D scaatter plot - can not read( can not see how many points are there for each class)
Hence we use Histogram for single feature instead of 1D plot 
Histogam - y axis says how mnay points exist -  smooth form inside a histogram is PDF (Probability Density function )
CDF - cumulative Density Function ( Remember it is cumulative )
Imagine PDF and CDF 

Mean | Variance | Standard deviation 
Median  -  sort the values - and take the middle value ( Even and ODD no careful )
Median is robust to outliers 
if more than 50% of values are corrupted or outlier, then median is corrupted.
1, 1.1 , 1.2, 5, 10,20, 30 
what is median absolute deviation(MAD)  -  it is robust 

Perctiles: Percentages of points 
50th percintile say is 50% value is less than that value
basicallt 50th percitile is median
10th percintile is 105 value is less than that point. 25th 50th 75th percintiles are called Quartiles.

Box plot:  quartiles plot. 25th - 50th - 75th , Whiskers - shows the outliers
IQR = Q3 −  Q1
Q1 − 1.5 IQR or above Q3 + 1.5 IQR  are outliers or whiskers

Violin plot - looks like violine 
box plot plus Histogram and PDF is Violine plots

Multivariate Probability Density plot :
joint plot in seaborne, Contour plot 


Linear Algebra
2d - line
3d - plane , more than 3d it is hyperplane
Understand distance .  root{(x2^2 -x1^2) + (y2^2 - y1^2) + (z2^2 - z1^2)}  - 3D distance 
if through origine - one point will be zero

Distance:
Eucledian Distance
Minkowski distance | 1-norm distance	| 2 norm | n - norm 
Manhatten distance
Cosine distance
Levenshtein distance
Hamming distance

row vector and column vector. Default is column vector 
Projection 
Unit vector  a/||a||

Equation of a line  - ax+by+c = 0   |  wTX+w0 = 0 | y = mx+c 
plane = ax+by+cz+w0 = 0
hyperpalce  w1x1 + w2x2+w3x3+...+wnxn +w0 = 0

passing through origine means  c = 0 ( Y-intercept)

Circle: in 2d - x1 and x2 

x1^2 + x2^2+ ...+ Xn^2 = r^2 

Ellcipse equation: x1..xn
x^2/a^2 + y^2/b^2 = 1
-------------------
Random variable X
Population, sample

Gaussian Distribution 
learn the equation 
68-95-99 rule 
 0 1 2 3 sigma( std)

symmetric distribution
Right skewed left skewed  ;  Kurtosis - peak 

standardize- x-mu/sigma   Z = N(0,1)  means 0 ,  1 std

Kernel Density Estimation :  density of ditribution . 

Sampling distribution : take sample from population  and see 

Central Limit Theorem:  take a random variable X , we do not know the distribution 
take the samples  at least 30 . Each sample has let say 1000 points.
callculate the sample mean for each sample

all sample means has a distribution . Draw the distributions. It follows Normal distibutions with same mean( mu) as population
Variance will be  Variance/n    -  n is no of samples. n = 30 .

Q-Q Plot: Quantile Quantile plot
Is X Gaussin distributes -  Q-Q plots tell us
KS test, Anderson darling test also tells the sam 

Q-Q plot is graphical method.
sort them  and compute percentile  

Create radom variable Y whihc is Gaussian distribution.  Compute the percantile
Plot X -persantile and Y - percantile . If both are straight line - it is Gaussin dist.

data = np.random.normal(loc = 0,scale =1,size=1000) # loc is mean, scale  = variance 
stas.probblot(data,dist = norm, plot = pylab)

How Distribution are used: if we know mu(mean) and sigma(std) then we can plot PDF and CDF.

Chebysev's in equality:
For Guassian distrubution we can assume 68-95-99.7 rule
but if we do not know the distribution - but know mu and sigma , then we can say some % percentage of lies between 2/3/4  sigma

at least 1 − 1/k2 of the distribution's values are within k standard deviations of the mean. 2sigma, k=2 , 1-1/4 = 3/4 = 75%

Discrete & Continuous Uniform distribution:
for discrete - we call Probability Mass function 
for continuous - we call PDF 
Application of Uniform distribution :
How to generate uniform random sample - random.random  

Bernoulli and Binomial distribution : both are discrete dist 

Bernouli- coin toss  p+q = 1

Binomial Distribution: coin toss n times,

pmf  =(n,k)p^k(1-p)^n-k

Log Normal distribution: if log(x) is normally distributed then we call x is log normally distributed
Application - length of comments posted in internate

Power law distribution :
Pareto distribution - 

BOX-COX tarnsformation .

y = x^lambda - 1 / lambda if lambda not 0
    log(x) if lambda = 0 
    
stats 

Weibull dist:

Relationship between random variable:

Covariance : cov(x,y)  = submession{ ( x = mean(x) ) ( y - mean(y) ) } / n

pearson correlation  -  Cov(x,y) / std(x) std(y)

It can not determine Non linear relationship

Spearman Rank correlation Coefficient: -  It can read Non-linear relationship.

create rank of x  - sort it and give rank

x  y   rx   ry

compute pearson corr coef for rank(x) and rank(y)

Correlation vs Causation:

Confidence Interval: Point estimate is not always right.

if we give range/interval (162 , 175) with 90% probability

for gaussuan distribution - mean = 168 and std= 5 
(mean - 2*std)  and  (mean + 2*std)  lies 95% observation  . this is CI

CI using Bootstraping :
Bootstraping: Sampling with replacement 

Non - parametric tech - does not make any assumptions about the distribution of data.
Bootstrap is non-parametric

Hypothesis testing:
------------------
it follows proof by contradiction 
it is statistical inference tech
2 datasets are compared,  datasets are obtained by sampling
Null hypothesis - propses no relationship among datasets
Alternate hypothesis - relationship between datasets
Significance level  - alpha - 0.05 or 0.01
Type 1 and Type 2 errros

steps:
1.state the relevant null and alternative hypotheses
2.Decide which test is appropriate, and state the relevant test statistic T.  
 test statistics  - follow normal distribution or t-distributions
3. calculate the p- value

Test Statistic:  is the testing process 
 a.Chi- squared test
 b. t -student test
 c.Fishers z-test  etc.
 
Reject the null hypothesis, in favor of the alternative hypothesis,
if and only if the p-value is less than (or equal to) the significance level (the selected probability) threshold .

P-value is ( observation by experiment / Assumptions) . let say it is 3%  , less than 5%  , Alternate hypothesis is true
p(obs/H0) < 5%

for observation we use -Test statistics

K- S test:
compare 2 random variable have same distributions .?
compare CDF of 2 random variable , 
stats.kstest(x, 'norm') - for normal distribution
KS test uses hypothesis testing

Proportional Sampling :
P S is used when the population is composed of several subgroups that are vastly different in number.
The number of participants from each subgroup is determined by their number relative to the entire population.

 imagine you want to create a council of 20 employees that will meet and recommend possible 
 changes to the employee handbook.
 Let's say 40% of your employees are in Sales and Marketing, 30% in Customer Service, 
 20% of your employees are in IT, and 10% in Finance. You will randomly select 8 people from Sales and Marketing, 
 6 from Customer Service, 4 from IT, and 2 from Finance. 
 As you can see, each number you pick is proportionate to the 
 overall percentage of people in each category (e.g., 40% = 8 people).

Dimensionality Reduction:

row vector & column vector(default)

Feature Normalization/Column normalization

ai = ai - min(a) / max(a) - min(a)

Covariance matrix:
----------------
Cov(x,y) = Cov(y,x)

given a matrix  n * d where d is the no of features.
the covariance matrix of X  (n *d) is S (d*d) - square matrix 
calculate covariance of each col with other col

PCA:
https://builtin.com/data-science/step-step-explanation-principal-component-analysis
1.standardize variables so that each one of them contributes equally to the analysis.
2.Covariance matrix computations
 sometimes, variables are highly correlated in such a way that they contain redundant information.
 So, in order to identify these correlations, we compute the covariance matrix.
3.COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS

principal components:
So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible
information in the first component, then maximum remaining information in the second and so on.
principal component accounts for the largest possible variance in the data set
 
it is eigenvectors and eigenvalues who are behind all the magic explained above, because the eigenvectors of the
Covariance matrix are actually the directions of the axes where there is the most variance(most information).
and that we call Principal Components.
And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in 
each Principal Component.
explained vaariance

Limitation of PCA:-  It is not for Non-linear data 
PCA preserve global shaoe of data. It does not preserve local distribution into consdiretation

T-SNE: t-distributed  stochastic neighbourhood embedding
Neighbourhood  - geometrically close - distance between points are close
embedding - Convert d dimension poitns  to 2d(lower dimension)  via projection 

preserve distances of points in neighbourhood
crowding problem in t-sne :
It is impossible to preserve distances in all the neighbourhood.
poits are getting crowded.

to avaoid crowding problem we use t-distibution

T-sne is iterative algo and randomized
paramters: stepsize = no of iterations
perplexity -  no of neighbours/points it should consider 
5,100, 1000 - consider 1000 points a neighbourhood . Keep increasing and check whihc one stable
-------------------------
Supervised:
Support Vector machine:
Maximal Margin Hyperplane 
Support vectors | Margin 














































