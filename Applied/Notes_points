Exploratory data analysis
imbalance & Balanced dataset

2d scatter plot 
3d Scatter plot | what about 4d 5d - can human visualize ?- no
1D scaatter plot - can not read( can not see how many points are there for each class)

pair plot  -  seaborne  -  multiple plots for each pair of columns . ignore diagonal element 
Write observation after plotting -  simple sentence - numeric value
more than 10 features - difficult to visualize and takes time

Univriate Analysis:
Histograms - PDF , CDF , Bins ( Binning )
1D scaatter plot - can not read( can not see how many points are there for each class)
Hence we use Histogram for single feature instead of 1D plot 
Histogam - y axis says how mnay points exist -  smooth form inside a histogram is PDF (Probability Density function )
CDF - cumulative Density Function ( Remember it is cumulative )
Imagine PDF and CDF 

Mean | Variance | Standard deviation 
Median  -  sort the values - and take the middle value ( Even and ODD no careful )
Median is robust to outliers 
if more than 50% of values are corrupted or outlier, then median is corrupted.
1, 1.1 , 1.2, 5, 10,20, 30 
what is median absolute deviation(MAD)  -  it is robust 

Perctiles: Percentages of points 
50th percentile say is 50% value is less than that value
50th percentile is median
10th percentile is 10% value is less than that point. 25th 50th 75th percentiles are called Quartiles.

Box plot:  quartiles plot. 25th - 50th - 75th , Whiskers - shows the outliers
IQR = Q3 −  Q1
Q1 − 1.5 IQR or above Q3 + 1.5 IQR  are outliers or whiskers

Violin plot - looks like violine 
box plot plus Histogram and PDF is Violine plots

Multivariate Probability Density plot :
joint plot in seaborne, Contour plot 


Linear Algebra
2d - line
3d - plane , more than 3d it is hyperplane
Understand distance .  root{(x2^2 -x1^2) + (y2^2 - y1^2) + (z2^2 - z1^2)}  - 3D distance 
if through origine - one point will be zero

Distance:
Eucledian Distance
Minkowski distance | 1-norm distance	| 2 norm | n - norm 
Manhatten distance
Cosine distance
Levenshtein distance
Hamming distance

row vector and column vector. Default is column vector 
Projection 
Unit vector  a/||a||

Equation of a line  - ax+by+c = 0   |  wTX+w0 = 0 | y = mx+c 
plane = ax+by+cz+w0 = 0
hyperplane  w1x1 + w2x2+w3x3+...+wnxn +w0 = 0

passing through origin means  c = 0 ( Y-intercept)

Circle: in 2d - x1 and x2 

x1^2 + x2^2+ ...+ Xn^2 = r^2 

Ellcipse equation: x1..xn
x^2/a^2 + y^2/b^2 = 1
-------------------
Random variable X
Population, sample

Gaussian Distribution 
learn the equation 
68-95-99 rule 
 0 1 2 3 sigma( std)

symmetric distribution
Right skewed left skewed  ;  Kurtosis - peak 

standardize- x-mu/sigma   Z = N(0,1)  means 0 ,  1 std

Kernel Density Estimation :  density of ditribution . 

Sampling distribution : take sample from population  and see 

Central Limit Theorem:  take a random variable X , we do not know the distribution 
take the samples  at least 30 . Each sample has let say 1000 points.
callculate the sample mean for each sample

all sample means has a distribution . Draw the distributions. It follows Normal distibutions with same mean( mu) as population
Variance will be  Variance/n    -  n is no of samples. n = 30 .

Q-Q Plot: Quantile Quantile plot
Is X Gaussin distributes -  Q-Q plots tell us
KS test, Anderson darling test also tells the sam 

Q-Q plot is graphical method.
sort them  and compute percentile  

Create radom variable Y whihc is Gaussian distribution.  Compute the percantile
Plot X -percentile and Y - percentile . If both are straight line - it is Gaussin dist.

data = np.random.normal(loc = 0,scale =1,size=1000) # loc is mean, scale  = variance 
stas.probblot(data,dist = norm, plot = pylab)

How Distribution are used: if we know mu(mean) and sigma(std) then we can plot PDF and CDF.

Chebysev's in equality:
For Guassian distrubution we can assume 68-95-99.7 rule
but if we do not know the distribution - but know mu and sigma , then we can say some % percentage of lies between 2/3/4  sigma

at least 1 − 1/k2 of the distribution's values are within k standard deviations of the mean. 2sigma, k=2 , 1-1/4 = 3/4 = 75%

Discrete & Continuous Uniform distribution:
for discrete - we call Probability Mass function 
for continuous - we call PDF 
Application of Uniform distribution :
How to generate uniform random sample - random.random  

Bernoulli and Binomial distribution : both are discrete dist 

Bernouli- coin toss  p+q = 1

Binomial Distribution: coin toss n times,

pmf  =(n,k)p^k(1-p)^n-k

Log Normal distribution: if log(x) is normally distributed then we call x is log normally distributed
Application - length of comments posted in internate

Power law distribution :
Pareto distribution - 

BOX-COX tarnsformation .

y = x^lambda - 1 / lambda if lambda not 0
    log(x) if lambda = 0 
    
stats 

Weibull dist:

Relationship between random variable:

Covariance : cov(x,y)  = submession{ ( x - mean(x) ) ( y - mean(y) ) } / n

pearson correlation  -  Cov(x,y) / std(x) std(y)

It can not determine Non linear relationship

Spearman Rank correlation Coefficient: -  It can read Non-linear relationship.

create rank of x  - sort it and give rank

x  y   rx   ry

compute pearson corr coef for rank(x) and rank(y)

Correlation vs Causation:

Confidence Interval: Point estimate is not always right.

if we give range/interval (162 , 175) with 90% probability

for gaussuan distribution - mean = 168 and std= 5 
(mean - 2*std)  and  (mean + 2*std)  lies 95% observation  . this is CI

CI using Bootstraping :
Bootstraping: Sampling with replacement 

Non - parametric tech - does not make any assumptions about the distribution of data.
Bootstrap is non-parametric

Hypothesis testing:
------------------
it follows proof by contradiction 
it is statistical inference tech
2 datasets are compared,  datasets are obtained by sampling
Null hypothesis - propses no relationship among datasets
Alternate hypothesis - relationship between datasets
Significance level  - alpha - 0.05 or 0.01
Type 1 and Type 2 errros

steps:
1.state the relevant null and alternative hypotheses
2.Decide which test is appropriate, and state the relevant test statistic T.  
 test statistics  - follow normal distribution or t-distributions
3. calculate the p- value

Test Statistic:  is the testing process 
 a.Chi- squared test
 b. t -student test
 c.Fishers z-test  etc.
 
Reject the null hypothesis, in favor of the alternative hypothesis,
if and only if the p-value is less than (or equal to) the significance level (the selected probability) threshold .

P-value is ( observation by experiment / Assumptions) . let say it is 3%  , less than 5%  , Alternate hypothesis is true
p(obs/H0) < 5%

for observation we use -Test statistics

K- S test:
compare 2 random variable have same distributions .?
compare CDF of 2 random variable , 
stats.kstest(x, 'norm') - for normal distribution
KS test uses hypothesis testing

Proportional Sampling :
P S is used when the population is composed of several subgroups that are vastly different in number.
The number of participants from each subgroup is determined by their number relative to the entire population.

 imagine you want to create a council of 20 employees that will meet and recommend possible 
 changes to the employee handbook.
 Let's say 40% of your employees are in Sales and Marketing, 30% in Customer Service, 
 20% of your employees are in IT, and 10% in Finance. You will randomly select 8 people from Sales and Marketing, 
 6 from Customer Service, 4 from IT, and 2 from Finance. 
 As you can see, each number you pick is proportionate to the 
 overall percentage of people in each category (e.g., 40% = 8 people).

Dimensionality Reduction:

row vector & column vector(default)

Feature Normalization/Column normalization

ai = ai - min(a) / max(a) - min(a)

Covariance matrix:
----------------
Cov(x,y) = Cov(y,x)

given a matrix  n * d where d is the no of features.
the covariance matrix of X  (n *d) is S (d*d) - square matrix 
calculate covariance of each col with other col

PCA:
https://builtin.com/data-science/step-step-explanation-principal-component-analysis
1.standardize variables so that each one of them contributes equally to the analysis.
2.Covariance matrix computations
 sometimes, variables are highly correlated in such a way that they contain redundant information.
 So, in order to identify these correlations, we compute the covariance matrix.
3.COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS

principal components:
So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible
information in the first component, then maximum remaining information in the second and so on.
principal component accounts for the largest possible variance in the data set
 
it is eigenvectors and eigenvalues who are behind all the magic explained above, because the eigenvectors of the
Covariance matrix are actually the directions of the axes where there is the most variance(most information).
and that we call Principal Components.
And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in 
each Principal Component.
explained vaariance

Limitation of PCA:-  It is not for Non-linear data 
PCA preserve global shaoe of data. It does not preserve local distribution into consdiretation

T-SNE: t-distributed  stochastic neighbourhood embedding
Neighbourhood  - geometrically close - distance between points are close
embedding - Convert d dimension poitns  to 2d(lower dimension)  via projection 

preserve distances of points in neighbourhood
crowding problem in t-sne :
It is impossible to preserve distances in all the neighbourhood.
points are getting crowded.

to avaoid crowding problem we use t-distibution

T-sne is iterative algo and randomized
paramters: stepsize = no of iterations
perplexity -  no of neighbours/points it should consider 
5,100, 1000 - consider 1000 points a neighbourhood . Keep increasing and check whihc one stable
-------------------------
NLP
--------------
Data cleaning and duplication 
Text to vector: why to vector - we can use all math in linear algebra
how do we convert text to vector: BOW,tf-IDF,w2v,avg w2v,tf-idf w2v 
text -> d dimensional vector , Similar text to be closer geometrically
BOW: Bag of words: simple 
- find set of all unique words in the corpus, and create a vector . let say  d unique words. so vector size is d
ex: this pasta is very tasty=> whichever word is there put 1, others 0.this way we can create a vector.
    this pasta is not tasy
  distance between word is very small. But meaning is completwly opposite
- sparce vector
corpus is collection of documents
documets is each sentence
similar text must have closer vector

It just count the words 
     
Binary BOW - just use 0 or 1 
BOW - Count the no words in a document 

Stop word removal | Tokenization | Lemmatization 

not is stopwords ,  if we remove NOT then sentence changes 
so removing stop words is not usueful always. 

Make everything lower case 
Stemming- stem words porterstemmer, snowballstemmer 
Lemmatization - 

semantic meaning of words- word 2 vec  ( tasty & Delicious ) BOW create 2 different vector

Unigram , Bigram , ngrams
bigrams - pairs of word takes to consideration while building vectors 

BOW - complete discard sequece information. When we use Bi,Trigrams we can preserve some info.

for Bi,tri,ngram, the dimensionality increases .compared to unigram 

TF-IDF : Term frequence , Inverse document  frequency 

r1 - w1, w2 w3 w4 w5 w6 
r2- w1 w3 w4 w5 w6 w8

tf(wi, rj) =  no of times wi in rj / total no of words in rj

D- corpus 
IDF(Wi, D)  = log ( N/ni) 
N= no of documents,  ni = no of documents which contais wi

log( N/ni) > 0 

if wi is more frequent, then IDF decreases.  If wi is mor frequent, then IDF is low for that word

TF(wj,rj) *IDF(wj,D) 

Why do we use LOG in IDF ?

zipf's law
Histograms of words in english  powerlaw distribution
provide smooth ness 
TO make TF and IDF in same scale we take log in IDF, TF is  0 -1 .

Word 2 vec :- Take semantic meaning 
dense vector for a word ,

300 dimensional vector : for a word :more dimension, more information rich 
w1 and w2 are sematically similar, then v1 and v2 distance is less

google news  -  w2v

neighbourhood of words 

Average word 2 vector :  documents are sequence of words
Convert sentences to a vector using W2V.

w2v(w1) + w2v(w2) + .... / no fo words(n)   =  vector representation of r1 (douments )

not always works well

tf-idf w2v: 
calculate the TF-IDF vector for r1 
calculate w2v for w1
multiply  tf-idf(w1) *w2v(w1)  + tf-idf(w2)*w2v(w2) + ..../ sum of ( tf-idf) 

it is weighted average 

Sentence --> Vectros 
Thought Vector 

Preprocessing   - Regualr expression

gensim 
---------------------------
K-nearest neighbour - 

neighbourhood - geometrically proximity
K - how to pick 
Diatnce between points
Eucledian, Manhatten, Cosine, Hamming 

cosine distance  =  i- cosine similarity

Decision surfaces 
Ovrfitting and Underfit 
Need for cross validation
K-Fold cross validation 

Visualize Train , cross validation and test datasets

How to determone Overfit and Underfit :

plot error vs K in Knn.

Time based splitting : 

Weighted Knn -  gives weights to each nearest neighbor
More weight to close points


Vornoi diagram : 

KD-tree -  need to reduce KNN time complexity (n) to lower.
hence we use KD-tree

If dimension is more KDtree does not work
Also data is not uniform , Then also it does not work.

BALL TREE: Variations of KD tree

Locality Sensitive hashing : LSH

Using hash function, the points which are close go to same bucket. table 

Locality sensitive hashing using Cosine similarity: 
LSH using Euclidean distance

Probabilistic class label in KNN :
Mechgrid -  plot the graph in KNN

Imbalanced data set bs Balanced 
----
1. Undersampling ( Wastagae of data is consequence )
2. Oversampling (  100 original point to 900 poitns , Repeating points ) 
3. Syntehtic points -  Create new ponts with the minority region points 
    it is called extrpolations 
    
 4. Class wights concept -  minoroty points - give more weights 
 
Multiclass Classification : 
Run binary classfier multiple times .  N binary classifier 

Similarity Matrix : 

Train and Test set difference : How to determine 
Use any binary classifier  with modified datset.

Convert D-train to positive class label and 
D-test into a negative class label.
Apply K-NN or any binary classifier 

Impact of Outliers:

Local Outlier factor  - detects outlier 
based on KNN

Compute Local density 

K-Disatance  =  distance of Kth nearest neighbor of Xi from Xi

5th distance of Xi, distance of 5th Nearest neighbor 

Neighborhood of X 

Reachablity Distance : 

Local reachability Density : 

Average reach distance 

For each point Xi compute LOF(xi)

local outlier probability

LOF uses idea of Neighbourhood


Scale and Column Standardization :-
--------------

x' = x-mean(x) / std(x)

Model Intepretability vs Black box model  :


Feature importance and Forward feature selection 
Backward feature ellimination 

Categorical and ordinal feature:
1.give a number to Categorical feature -  many times fail 
2. One - hot encoding  - Dimensiaonlity increases 
3. Domain knowledge
It is completely problem specific . No right/wrong  

4. Mean replacement.  Use mean value of Yi for each category - This way we can convert Categorical to Numerical Feature.
Average Age for country India  - 156
Average age for country US  = 170
Replace India = 156 , Replace US = 170

Handling Missing Values:
--------------------
1. Imputation  -  replce the NaN value with Mean|Median| mode (Most Frequent )
2. Mean Imputation based on class label .  ( for certain class the there is NaN value in Other columns)
 take mean replacement of  that columns where  class label is 1 or 0 
3. Impute the missing value with above strategy and Add new column saying this value is missing .
   adding missing value features
4. Model based  Imputations .  Use KNN or other to fill the missing value. COnvert it to standard classificatio or rgression problem 
 
Curse of Dimensionality:
-----------
Eucledian distance  - is not valid in multidimensional space.
Solution is Cosine distance  - Use in text processing ( Impact is less ) 

high dimension and Dense - impact of high dimension is high
high dimension and sparce  - impact of high dimesion is less

Bias Variance Tradeoff :
-------------
Gen_error   = bias + variance + irreducible error 
Gen_error should be low

Train error and test error

Train error high - High bias 

Test error is high, train error is low - High vairance 

Best and Worst case algorithm:
--------------------------
Low latency system 
choose right distance measure


Accuracy:- Measure the performance of model 
--------
Never use accuracy as a measure when we have Imbalance dataset
We can use log loss- when probality gives probability score

Confusion Matrix:
--------
  A   1     0 
P
1     

0

Total Positve  =  TP + FN
Total Negative =  TN + FP

TPR = True Positive rate =  TP/Total Postive
TNR =  TN/ Total Negative 

FPR = FP/ Total Negative

FNR = FN/Total Positive

FN in diseas detection is etremely dangerous, 

Precision| Recall | F1 score :

precision = TP/TP+FP  - This does not care of negative class
Recall =  TP/TP+FN    - 

F1 Score  = 2 * Precision* Recall / Precision + Recall

Receiver Operating Characteristics : AUC- Area Under  Curve
-----
TPR vs FPR  curve in  TPR in Y axis 
Useful for binary classification
AUC is not dependent on Y' score.

Log loss:
--------
- 1/n Submession ( y log y' + (1-y) log (1-y') )


R2 error:
-------
Regression problem

SS total =  submession {Y - avg(Y)}^2
SS res = submession { ( Y- Y' )^2 }

R2 = 1 - ( SS_res/SS_tot )

Median Absolute deviation: MAD
--------
Regression metrics 


Distribution of Error:
--

Naive Bayes:
----------
Conditional Probability

Independent Events and Mutually exclusive events

Laplace Smoothing:Additive smoothing-  alpha
----

Log Probabilities:
-----
log( p( y| w1,w2...wn) ) 
compare log probabilities 
Log convert multiplication to addition
 log(a*b) = log a+ log b
 
Feature importance in Naive Bayes:

------------------

W* = argmax(YW^TX)  

Find Optimal W*












