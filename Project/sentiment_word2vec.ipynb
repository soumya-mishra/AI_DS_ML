{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have taken 100 rows from my input text . Takes lot of time if i take 25k row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c4b588ff-c7d5-407a-b4b6-80e8bbc7690c",
    "_uuid": "9112bbd7f06e0dade3180b209ad38ebcf178d909"
   },
   "outputs": [],
   "source": [
    "# Firstly, please note that the performance of google word2vec is better on big datasets. \n",
    "# In this example we are considering only 25000 training examples from the imdb dataset.\n",
    "# Therefore, the performance is similar to the \"bag of words\" model.\n",
    "\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# BeautifulSoup is used to remove html tags from the text\n",
    "from bs4 import BeautifulSoup \n",
    "import re # For regular expressions\n",
    "\n",
    "# Stopwords can be useful to undersand the semantics of the sentence.\n",
    "# Therefore stopwords are not removed while creating the word2vec model.\n",
    "# But they will be removed  while averaging feature vectors.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "train = pd.read_csv(r\"C:\\Users\\I324158\\Downloads\\testData.tsv\\testData.tsv\", header=0,\\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.iloc[0:100,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a text to a sequence of words.\n",
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word2vec expects a list of lists.\n",
    "# Using punkt tokenizer for better splitting of a paragraph into sentences.\n",
    "\n",
    "import nltk.data\n",
    "#nltk.download('popular')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits a review into sentences\n",
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_sentences(review, tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naturally',\n",
       " 'in',\n",
       " 'a',\n",
       " 'film',\n",
       " 'who',\n",
       " 's',\n",
       " 'main',\n",
       " 'themes',\n",
       " 'are',\n",
       " 'of',\n",
       " 'mortality',\n",
       " 'nostalgia',\n",
       " 'and',\n",
       " 'loss',\n",
       " 'of',\n",
       " 'innocence',\n",
       " 'it',\n",
       " 'is',\n",
       " 'perhaps',\n",
       " 'not',\n",
       " 'surprising',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'rated',\n",
       " 'more',\n",
       " 'highly',\n",
       " 'by',\n",
       " 'older',\n",
       " 'viewers',\n",
       " 'than',\n",
       " 'younger',\n",
       " 'ones']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(sentences)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-11 16:39:10,866 : INFO : collecting all words and their counts\n",
      "2020-01-11 16:39:10,867 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-01-11 16:39:10,889 : INFO : collected 4661 word types from a corpus of 23423 raw words and 991 sentences\n",
      "2020-01-11 16:39:10,891 : INFO : Loading a fresh vocabulary\n",
      "2020-01-11 16:39:10,899 : INFO : effective_min_count=40 retains 78 unique words (1% of original 4661, drops 4583)\n",
      "2020-01-11 16:39:10,901 : INFO : effective_min_count=40 leaves 11476 word corpus (48% of original 23423, drops 11947)\n",
      "2020-01-11 16:39:10,903 : INFO : deleting the raw counts dictionary of 4661 items\n",
      "2020-01-11 16:39:10,905 : INFO : sample=0.001 downsamples 78 most-common words\n",
      "2020-01-11 16:39:10,906 : INFO : downsampling leaves estimated 3736 word corpus (32.6% of prior 11476)\n",
      "2020-01-11 16:39:10,908 : INFO : estimated required memory for 78 words and 300 dimensions: 226200 bytes\n",
      "2020-01-11 16:39:10,910 : INFO : resetting layer weights\n",
      "2020-01-11 16:39:10,921 : INFO : training model with 4 workers on 78 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-01-11 16:39:10,936 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-11 16:39:10,942 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-11 16:39:10,944 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-11 16:39:10,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-11 16:39:10,946 : INFO : EPOCH - 1 : training on 23423 raw words (3726 effective words) took 0.0s, 291078 effective words/s\n",
      "2020-01-11 16:39:10,963 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-11 16:39:10,974 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-11 16:39:10,977 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-11 16:39:10,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-11 16:39:10,978 : INFO : EPOCH - 2 : training on 23423 raw words (3654 effective words) took 0.0s, 175449 effective words/s\n",
      "2020-01-11 16:39:10,989 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-11 16:39:10,995 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-11 16:39:10,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-11 16:39:11,000 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-11 16:39:11,001 : INFO : EPOCH - 3 : training on 23423 raw words (3863 effective words) took 0.0s, 256862 effective words/s\n",
      "2020-01-11 16:39:11,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-11 16:39:11,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-11 16:39:11,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-11 16:39:11,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-11 16:39:11,032 : INFO : EPOCH - 4 : training on 23423 raw words (3681 effective words) took 0.0s, 199290 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-11 16:39:11,045 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-11 16:39:11,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-11 16:39:11,051 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-11 16:39:11,054 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-11 16:39:11,055 : INFO : EPOCH - 5 : training on 23423 raw words (3654 effective words) took 0.0s, 263458 effective words/s\n",
      "2020-01-11 16:39:11,056 : INFO : training on a 117115 raw words (18578 effective words) took 0.1s, 139927 effective words/s\n",
      "2020-01-11 16:39:11,058 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-01-11 16:39:11,059 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-01-11 16:39:11,081 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2020-01-11 16:39:11,082 : INFO : not storing attribute vectors_norm\n",
      "2020-01-11 16:39:11,083 : INFO : not storing attribute cum_table\n",
      "2020-01-11 16:39:11,097 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-11 16:39:17,723 : WARNING : vectors for words {'man', 'dog', 'child', 'woman', 'kitchen'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot select a word from an empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8cb01d3c5d43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Few tests: This will print the odd word among them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoesnt_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"man woman dog child kitchen\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mdoesnt_match\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vectors for words %s are not present in the model, ignoring these words\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignored_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mused_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot select a word from an empty list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mused_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot select a word from an empty list"
     ]
    }
   ],
   "source": [
    "# Few tests: This will print the odd word among them \n",
    "model.wv.doesnt_match(\"man woman dog child kitchen\".split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-11 16:39:32,988 : WARNING : vectors for words {'berlin', 'england', 'france', 'germany'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot select a word from an empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-efa25cecbbc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoesnt_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"france england germany berlin\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mdoesnt_match\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vectors for words %s are not present in the model, ignoring these words\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignored_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mused_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot select a word from an empty list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mused_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot select a word from an empty list"
     ]
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'man' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4be3ac82382d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This will print the most similar words present in the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"man\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'man' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# This will print the most similar words present in the model\n",
    "model.wv.most_similar(\"man\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'awful' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c00a4c416973>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"awful\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'awful' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will give the total number of words in the vocabolary created from this dataset\n",
    "model.wv.syn0.shape\n",
    "# set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.61664037e-02,  8.24532807e-02,  3.76179861e-03,  5.81793138e-04,\n",
       "        3.64488550e-02,  5.70453629e-02,  6.94968458e-03,  9.85511839e-02,\n",
       "        3.93022858e-02,  4.59852293e-02, -1.03451647e-01,  1.65633857e-02,\n",
       "       -4.80380058e-02,  8.45082924e-02, -4.23093960e-02, -7.86713809e-02,\n",
       "       -1.45126462e-01,  1.10818952e-01, -2.29660925e-02,  4.75815535e-02,\n",
       "       -2.11016741e-02,  2.50058789e-02,  4.41681854e-02, -4.76989802e-03,\n",
       "       -5.67844845e-02,  3.12639922e-02,  2.37833560e-02,  2.79293414e-02,\n",
       "        2.40612868e-02,  4.30039410e-03,  9.33803394e-02,  1.56919323e-02,\n",
       "       -7.38396868e-02,  3.92112508e-03,  4.32204232e-02, -8.48979130e-02,\n",
       "        7.38167986e-02,  7.16893524e-02,  3.33599672e-02, -5.46611957e-02,\n",
       "        4.71505560e-02, -8.08743862e-05,  4.48456109e-02, -2.93611782e-03,\n",
       "       -1.40388580e-02, -6.77136406e-02, -3.75110917e-02, -5.26984185e-02,\n",
       "       -5.28386608e-02,  3.98561135e-02,  4.44052145e-02,  3.43946703e-02,\n",
       "       -5.58889396e-02, -5.16674668e-02,  4.43127714e-02,  2.38482263e-02,\n",
       "       -5.10291159e-02,  1.25064492e-01,  9.92997363e-02, -7.75911659e-02,\n",
       "        1.66325062e-03,  6.86406046e-02,  4.39252965e-02, -6.13297410e-02,\n",
       "        4.39934507e-02, -3.42379883e-02, -1.26206810e-02, -3.68876942e-02,\n",
       "       -7.84713477e-02, -2.56471671e-02,  9.56338048e-02, -8.73113051e-03,\n",
       "        9.97695848e-02,  1.04961703e-02,  5.13689108e-02,  4.21835706e-02,\n",
       "       -1.98226534e-02, -7.20794825e-03,  7.10394233e-02, -1.68097857e-02,\n",
       "        1.86689477e-02, -5.49274981e-02,  1.26939565e-01, -8.20389614e-02,\n",
       "       -7.58293867e-02, -1.20772077e-02, -4.92871590e-02, -3.63407508e-02,\n",
       "        2.85215359e-02, -3.05531807e-02, -2.05509365e-02,  2.06142459e-02,\n",
       "       -1.77364368e-02,  1.39351282e-02,  1.84627846e-02,  4.75270674e-02,\n",
       "       -7.29691535e-02,  9.06713307e-02,  2.25029681e-02,  2.50296704e-02,\n",
       "       -1.57233980e-02, -5.61322011e-02,  5.75565081e-03,  1.95738152e-02,\n",
       "       -8.05238485e-02, -3.60889398e-02,  5.98881058e-02, -4.09807786e-02,\n",
       "        5.51724471e-02,  7.39096031e-02,  8.58465359e-02,  1.46543449e-02,\n",
       "       -9.30814259e-03, -2.13363208e-02, -3.84470634e-02,  1.30927246e-02,\n",
       "       -1.61642339e-02, -1.42567360e-03,  1.46558926e-01,  7.88100362e-02,\n",
       "        6.10823333e-02,  3.45933251e-02,  3.85934766e-03,  2.02073921e-02,\n",
       "        6.12980612e-02, -2.65163258e-02,  5.99480905e-02, -9.08206999e-02,\n",
       "       -2.54736878e-02,  9.85044315e-02,  1.04991913e-01,  4.43120440e-03,\n",
       "        4.27314341e-02,  1.27029326e-02,  1.27146272e-02, -9.21848193e-02,\n",
       "       -1.34852286e-02, -2.72840615e-02,  1.12516224e-01, -6.04877025e-02,\n",
       "       -7.92179182e-02,  6.26830012e-02, -8.39133281e-03,  4.60505523e-02,\n",
       "       -4.68824282e-02, -5.49286231e-02,  4.46339361e-02,  2.46325508e-02,\n",
       "        2.08864454e-02,  6.59301365e-03,  1.39405439e-02,  1.03679575e-01,\n",
       "        2.54078824e-02,  3.39003839e-02, -1.55940102e-02,  1.00987211e-01,\n",
       "       -5.45331277e-02, -4.81681526e-03, -3.34853344e-02,  1.57760996e-02,\n",
       "        3.13531957e-03, -5.93217053e-02, -6.08688444e-02,  7.11161084e-03,\n",
       "       -1.20455958e-01,  2.93906480e-02, -1.17416382e-02, -5.29117361e-02,\n",
       "       -2.18985006e-02,  5.37967607e-02, -3.76340970e-02, -6.87600449e-02,\n",
       "       -6.71910495e-02,  1.53439455e-02,  3.13582309e-02,  1.86013617e-02,\n",
       "       -3.82931754e-02,  3.81781198e-02,  5.21567464e-03, -1.65115774e-03,\n",
       "        6.17037006e-02,  4.53111483e-03,  1.02915987e-01,  7.23964674e-03,\n",
       "       -4.08112593e-02,  9.82644930e-02, -4.24712189e-02, -3.85112641e-03,\n",
       "        2.86212284e-03,  5.14126522e-03, -4.78576832e-02, -1.74020790e-02,\n",
       "        6.37206957e-02, -2.35708281e-02,  9.05774906e-02,  8.33444893e-02,\n",
       "        3.61719839e-02,  9.45926160e-02, -4.71208990e-02,  4.19574156e-02,\n",
       "       -5.44650331e-02,  3.96317653e-02, -3.69040365e-03, -3.88097800e-02,\n",
       "        4.77845408e-02, -7.25596920e-02, -6.39474019e-02,  5.54792285e-02,\n",
       "       -6.89022392e-02,  7.44302664e-03, -2.30346229e-02, -1.74904522e-02,\n",
       "        2.31232140e-02, -4.51605208e-03,  4.33362275e-02, -8.42961576e-03,\n",
       "       -3.06620896e-02,  2.29172804e-03,  6.46949857e-02,  8.36733542e-03,\n",
       "        1.72674488e-02,  9.97971445e-02,  7.81338364e-02, -4.56617326e-02,\n",
       "       -7.04813302e-02,  7.17419609e-02, -8.03820863e-02, -4.07403670e-02,\n",
       "        7.20807239e-02,  2.87557952e-02, -4.65251990e-02, -6.97202012e-02,\n",
       "       -1.43768620e-02,  5.76489791e-02, -1.43725127e-01, -1.07060045e-01,\n",
       "        7.32572451e-02,  5.85954152e-02,  5.80566637e-02, -5.09788990e-02,\n",
       "       -1.45539753e-02, -1.41288014e-02,  1.32089883e-01,  5.73534220e-02,\n",
       "       -6.01361692e-02,  6.70824721e-02,  1.91214215e-02, -7.80641288e-02,\n",
       "        5.39308414e-02,  3.18813771e-02, -4.69258316e-02,  7.45947892e-03,\n",
       "        3.86471450e-02, -3.98807935e-02,  4.30418253e-02, -1.75924579e-04,\n",
       "        3.91601061e-04,  2.11874694e-02, -5.67678437e-02, -2.46624090e-02,\n",
       "       -2.44093593e-02, -8.37146044e-02,  1.25460207e-01,  3.78071470e-03,\n",
       "        7.04024509e-02,  1.21556632e-02,  1.82457313e-01, -6.47516921e-02,\n",
       "       -3.38749401e-02, -1.19492926e-01, -1.02895091e-03, -1.19619351e-02,\n",
       "       -7.46150762e-02,  5.45157753e-02, -1.09038964e-01,  6.33374974e-02,\n",
       "       -4.68453728e-02, -1.44970054e-02,  3.91196273e-02, -1.99349746e-02,\n",
       "        1.33421674e-01, -1.03123806e-01, -8.09022691e-03, -3.71935628e-02,\n",
       "        1.90518107e-02, -9.54426214e-05, -3.26301195e-02,  2.63132062e-02,\n",
       "       -9.66610573e-03,  1.76437095e-01, -1.04884967e-01,  4.73442264e-02,\n",
       "        4.06624703e-03, -6.43712133e-02,  6.44287020e-02,  6.99535981e-02,\n",
       "        8.48509148e-02, -8.19796845e-02,  2.66928063e-03, -4.31286134e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'it',\n",
       " 'i',\n",
       " 'this',\n",
       " 'that',\n",
       " 's',\n",
       " 'as',\n",
       " 'with',\n",
       " 'but',\n",
       " 'for',\n",
       " 'was',\n",
       " 'movie',\n",
       " 'you',\n",
       " 'he',\n",
       " 'film',\n",
       " 'are',\n",
       " 'on',\n",
       " 't',\n",
       " 'his',\n",
       " 'not',\n",
       " 'one',\n",
       " 'have',\n",
       " 'be',\n",
       " 'at',\n",
       " 'all',\n",
       " 'there',\n",
       " 'by',\n",
       " 'her',\n",
       " 'who',\n",
       " 'from',\n",
       " 'they',\n",
       " 'so',\n",
       " 'if',\n",
       " 'like',\n",
       " 'some',\n",
       " 'has',\n",
       " 'just',\n",
       " 'an',\n",
       " 'or',\n",
       " 'story',\n",
       " 'when',\n",
       " 'can',\n",
       " 'very',\n",
       " 'what',\n",
       " 'about',\n",
       " 'my',\n",
       " 'out',\n",
       " 'see',\n",
       " 'she',\n",
       " 'more',\n",
       " 'me',\n",
       " 'do',\n",
       " 'we',\n",
       " 'good',\n",
       " 'really',\n",
       " 'well',\n",
       " 'would',\n",
       " 'had',\n",
       " 'into',\n",
       " 'which',\n",
       " 'movies',\n",
       " 'its',\n",
       " 'only',\n",
       " 'time',\n",
       " 'first',\n",
       " 'up',\n",
       " 'no',\n",
       " 'character',\n",
       " 'were',\n",
       " 'then',\n",
       " 'him',\n",
       " 'get']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "#             pdb.set_trace()\n",
    "#             print(model[word])\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "#         pdb.set_trace()\n",
    "    return reviewFeatureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 100\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "# clean_test_reviews = []\n",
    "# for review in test[\"review\"]:\n",
    "#     clean_test_reviews.append(review_wordlist(review,remove_stopwords=True))\n",
    "    \n",
    "# testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'review'], dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the sentiment values for test data and saving the results in a csv file \n",
    "result = forest.predict(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"output.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
